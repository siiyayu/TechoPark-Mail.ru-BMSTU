{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TED-Talks\" data-toc-modified-id=\"TED-Talks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TED Talks</a></span></li><li><span><a href=\"#Токенизация\" data-toc-modified-id=\"Токенизация-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Токенизация</a></span></li><li><span><a href=\"#Распределение-слов-по-частоте,-стоп-слова\" data-toc-modified-id=\"Распределение-слов-по-частоте,-стоп-слова-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Распределение слов по частоте, стоп слова</a></span></li><li><span><a href=\"#Приведение-к-нормальной-форме\" data-toc-modified-id=\"Приведение-к-нормальной-форме-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Приведение к нормальной форме</a></span></li><li><span><a href=\"#Выделение-коллокаций\" data-toc-modified-id=\"Выделение-коллокаций-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Выделение коллокаций</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#поиск-похожих\" data-toc-modified-id=\"поиск-похожих-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>поиск похожих</a></span></li><li><span><a href=\"#выделение-ключевых-слов\" data-toc-modified-id=\"выделение-ключевых-слов-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>выделение ключевых слов</a></span></li></ul></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Embeddings\" data-toc-modified-id=\"Sentence-Embeddings-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Sentence Embeddings</a></span></li></ul></li><li><span><a href=\"#Topic-Modeling\" data-toc-modified-id=\"Topic-Modeling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Topic Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Поиск-похожих-документов\" data-toc-modified-id=\"Поиск-похожих-документов-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Поиск похожих документов</a></span></li><li><span><a href=\"#Краткое-описание-документов-(суммаризация)\" data-toc-modified-id=\"Краткое-описание-документов-(суммаризация)-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Краткое описание документов (суммаризация)</a></span></li><li><span><a href=\"#Визуализация-тематической-модели\" data-toc-modified-id=\"Визуализация-тематической-модели-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Визуализация тематической модели</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install razdel\n",
    "# !pip install pyLDAvis\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:25:08.828691Z",
     "start_time": "2020-11-01T09:25:08.748449Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import razdel\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pymorphy2\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import plotly\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:47.593866Z",
     "start_time": "2020-10-31T21:32:47.589925Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TOKEN_PATTERN = \"[а-яё]+\"\n",
    "\n",
    "DATA_PATH = './data/ted_talks.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you are using colab\n",
    "# !mkdir ./data\n",
    "# !wget https://raw.githubusercontent.com/vadim0912/park_ML/master/lecture06/data/ted_talks.csv.gz -O $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:48.152840Z",
     "start_time": "2020-10-31T21:32:47.596668Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:48.158011Z",
     "start_time": "2020-10-31T21:32:48.154784Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:48.302987Z",
     "start_time": "2020-10-31T21:32:48.159478Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"ww9ClmCWBr0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:48.948069Z",
     "start_time": "2020-10-31T21:32:48.304824Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text.str.len().hist(bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.618580Z",
     "start_time": "2020-10-31T21:32:48.955870Z"
    }
   },
   "outputs": [],
   "source": [
    "min_talk_len = 750\n",
    "\n",
    "df = df[df.text.str.len() >= min_talk_len].reset_index(drop=True)\n",
    "\n",
    "df.text.str.len().hist(bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.624291Z",
     "start_time": "2020-10-31T21:32:49.620673Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.628907Z",
     "start_time": "2020-10-31T21:32:49.626131Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = df.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.640435Z",
     "start_time": "2020-10-31T21:32:49.631816Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Как же так?! Олег... Мы же в 18.00 договаривались встретиться:(\"\n",
    "\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.649838Z",
     "start_time": "2020-10-31T21:32:49.643220Z"
    }
   },
   "outputs": [],
   "source": [
    "print(re.findall(\"[а-яА-ЯёЁ]+\", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.657966Z",
     "start_time": "2020-10-31T21:32:49.653138Z"
    }
   },
   "outputs": [],
   "source": [
    "print(re.split(r\"[-\\s.,;!?]+\", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.664939Z",
     "start_time": "2020-10-31T21:32:49.660548Z"
    }
   },
   "outputs": [],
   "source": [
    "print([token.text for token in razdel.tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.672152Z",
     "start_time": "2020-10-31T21:32:49.667553Z"
    }
   },
   "outputs": [],
   "source": [
    "print(nltk.tokenize.casual_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:49.685622Z",
     "start_time": "2020-10-31T21:32:49.675652Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0][:255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:51.016734Z",
     "start_time": "2020-10-31T21:32:49.688409Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(TOKEN_PATTERN, text.lower())\n",
    "\n",
    "docs = [tokenize(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:51.048334Z",
     "start_time": "2020-10-31T21:32:51.018415Z"
    }
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распределение слов по частоте, стоп слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте словарь `слово` -> `количество вхождений в корпус`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:52.492202Z",
     "start_time": "2020-10-31T21:32:51.049966Z"
    }
   },
   "outputs": [],
   "source": [
    "occurence = Counter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:52.497911Z",
     "start_time": "2020-10-31T21:32:52.494471Z"
    }
   },
   "outputs": [],
   "source": [
    "len(occurence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:57:09.967303Z",
     "start_time": "2020-10-31T13:57:09.956743Z"
    }
   },
   "source": [
    "в словаре больше 150 тыс. слов, хотя в литературном русском около 150 тыс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:52.533352Z",
     "start_time": "2020-10-31T21:32:52.499988Z"
    }
   },
   "outputs": [],
   "source": [
    "occurence.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые частые слова относятся к служебным частям речи и несут мало полезной информации. Их можно отбрасывать с помощью подготовленных списков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:52.540863Z",
     "start_time": "2020-10-31T21:32:52.535276Z"
    }
   },
   "outputs": [],
   "source": [
    "stopword_set = set(nltk.corpus.stopwords.words('russian'))\n",
    "# stopword_set = stopword_set.union({'это', 'который', 'весь', 'наш', 'свой', 'ещё',  'её', 'ваш', 'также', 'итак'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:52.547630Z",
     "start_time": "2020-10-31T21:32:52.542615Z"
    }
   },
   "outputs": [],
   "source": [
    "stopword_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.542924Z",
     "start_time": "2020-10-31T21:32:52.549641Z"
    }
   },
   "outputs": [],
   "source": [
    "max_rank = 40_000\n",
    "\n",
    "counts = [count for _, count in occurence.most_common()[:max_rank]]\n",
    "\n",
    "plt.plot(range(1, len(counts) + 1), counts)\n",
    "plt.title('log-log scale', fontsize=18)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('rank', fontsize=16)\n",
    "plt.ylabel('count', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./zipf_law.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приведение к нормальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.683449Z",
     "start_time": "2020-10-31T21:32:53.545006Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(occurence.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.698454Z",
     "start_time": "2020-10-31T21:32:53.685540Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в словаре встречаются разные формы одного и того же слова:\n",
    "* 'аббревиатур',\n",
    "* 'аббревиатура',\n",
    "* 'аббревиатуре',\n",
    "* 'аббревиатурой',\n",
    "* 'аббревиатуру',\n",
    "* 'аббревиатуры'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.702960Z",
     "start_time": "2020-10-31T21:32:53.700419Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.708172Z",
     "start_time": "2020-10-31T21:32:53.704971Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    'аббревиатур',\n",
    "    'аббревиатура',\n",
    "    'аббревиатуре',\n",
    "    'аббревиатурой',\n",
    "    'аббревиатуру',\n",
    "    'аббревиатуры',\n",
    "    'человек',\n",
    "    'люди',\n",
    "    'людьми'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.713606Z",
     "start_time": "2020-10-31T21:32:53.709999Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.792350Z",
     "start_time": "2020-10-31T21:32:53.715825Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T02:16:03.389431Z",
     "start_time": "2020-11-02T02:16:03.362537Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.parse('стекло')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:32:53.807122Z",
     "start_time": "2020-10-31T21:32:53.802640Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:10.576715Z",
     "start_time": "2020-10-31T21:32:53.809273Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer_cache = {}\n",
    "\n",
    "def lemmatize(token):\n",
    "    if lemmatizer.word_is_known(token):\n",
    "        if token not in lemmatizer_cache:\n",
    "            lemmatizer_cache[token] = lemmatizer.parse(token)[0].normal_form\n",
    "        return lemmatizer_cache[token]\n",
    "    return token\n",
    "\n",
    "lemmatized_docs = [[lemmatize(token) for token in text] for text in tqdm_notebook(docs)]\n",
    "\n",
    "cleared_docs = [[token for token in text if token not in stopword_set] for text in lemmatized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:10.597851Z",
     "start_time": "2020-10-31T21:34:10.580949Z"
    }
   },
   "outputs": [],
   "source": [
    "cleared_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во сколько раз уменьшился словарь после лемматизации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение коллокаций\n",
    "\n",
    "Посчитайте, сколько раз пары слов встречались вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:14.287823Z",
     "start_time": "2020-10-31T21:34:10.791659Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurence = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:14.451157Z",
     "start_time": "2020-10-31T21:34:14.291429Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PMI(x,y) = \\log\\frac{p\\left(x,y\\right)}{p\\left(x\\right)p\\left(y\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w_i) \\sim \\frac{\\text{count}\\left(w_i\\right)}{\\displaystyle\\sum_{j=1}^n \\text{count}\\left( w_j \\right)} = \\frac{\\text{count}\\left(w_i\\right)}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PMI(w_1, w_2) \\sim \\log\\left(\\frac{N \\cdot \\text{count}\\left(w_1, w_2\\right)}{\\text{count}\\left(w_1\\right) \\text{count}\\left(w_2\\right) }\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T18:51:18.654645Z",
     "start_time": "2020-11-02T18:51:18.608034Z"
    }
   },
   "source": [
    "постройте словарь, в котором парам слов соответствует значения pmi\n",
    "\n",
    "добавляйте в словарь только те пары слов, которые встретились в корпусе хотя бы `min_cooccur` раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:15.072131Z",
     "start_time": "2020-10-31T21:34:14.454860Z"
    }
   },
   "outputs": [],
   "source": [
    "pmi = \n",
    "min_cooccur = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если хочется автоматически выделять коллокации для извлечения признаков: [gensim.models.phrases.{Phrases, Phraser}](https://www.machinelearningplus.com/nlp/gensim-tutorial/#10howtocreatebigramsandtrigramsusingphrasermodels)\n",
    "\n",
    "```python\n",
    "phrases = (bows, min_count=30, progress_per=500)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(phrases)\n",
    "\n",
    "bigram[bows[0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## поиск похожих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:16.077126Z",
     "start_time": "2020-10-31T21:34:15.090336Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x, lowercase=False, max_df=0.8, min_df=2\n",
    ").fit(cleared_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получите tf-idf векторы документов, посчитайте cosine similarity каждого документа с каждым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:18.981016Z",
     "start_time": "2020-10-31T21:34:18.114227Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "for i in range(similarities.shape[0]):\n",
    "    for j in range(similarities.shape[1]):\n",
    "        if j > i:\n",
    "            if similarities[i, j] > threshold:\n",
    "                print(f\"({i}, {j}) {similarities[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:18.986576Z",
     "start_time": "2020-10-31T21:34:18.982774Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_talks(talk_ids, text_chunk=1024):\n",
    "\n",
    "    for talk_id in talk_ids:\n",
    "        record = df.loc[talk_id]\n",
    "\n",
    "        print(record.talk)\n",
    "        print('-' * 100)\n",
    "        print(record.text[:text_chunk])\n",
    "        print('#' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:34:19.018242Z",
     "start_time": "2020-10-31T21:34:18.988753Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((815, 1199))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## выделение ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T21:02:07.722209Z",
     "start_time": "2020-11-01T21:02:07.716693Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_id = 815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выделите `top_k` ключевых слов для документа `doc_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:35:30.723018Z",
     "start_time": "2020-10-31T21:34:19.020461Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sentence_dataset(documents):\n",
    "    tokenized_sentences = []\n",
    "    for document in tqdm_notebook(documents):\n",
    "        for sentence in razdel.sentenize(document):\n",
    "            lemmatized_tokens = [lemmatize(token) for token in re.findall(TOKEN_PATTERN, sentence.text.lower())]\n",
    "            tokenized_sentences.append(\n",
    "                [token for token in lemmatized_tokens if token not in stopword_set]\n",
    "            )\n",
    "    return tokenized_sentences\n",
    "\n",
    "sentence_dataset = prepare_sentence_dataset(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:35:30.729968Z",
     "start_time": "2020-10-31T21:35:30.725923Z"
    }
   },
   "outputs": [],
   "source": [
    "len(sentence_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:35:30.736973Z",
     "start_time": "2020-10-31T21:35:30.731599Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(size=100, sg=0, window=5, min_count=5, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:35:34.933207Z",
     "start_time": "2020-10-31T21:35:30.738838Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentence_dataset, progress_per=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:35:34.939445Z",
     "start_time": "2020-10-31T21:35:34.935107Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:36:54.632140Z",
     "start_time": "2020-10-31T21:35:34.942227Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.train(sentence_dataset, total_examples=word2vec.corpus_count, epochs=30, report_delay=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T21:26:17.651222Z",
     "start_time": "2020-11-01T21:26:17.625984Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "test_words = ['альтруизм', 'бедность', 'платье', 'сентябрь', 'компьютер', 'лондон']\n",
    "\n",
    "for word in test_words:\n",
    "    print(word)\n",
    "    print(\n",
    "        tabulate(word2vec.wv.most_similar(word), tablefmt='orgtbl', headers=('neighbor', 'score')),\n",
    "        end='\\n\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:36:54.703427Z",
     "start_time": "2020-10-31T21:36:54.693526Z"
    }
   },
   "outputs": [],
   "source": [
    "index2word = np.array(word2vec.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:36:54.710895Z",
     "start_time": "2020-10-31T21:36:54.706439Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = word2vec.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обучите TSNE на выборке случайных слов для понижения размерности векторов до двух компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:36:54.721273Z",
     "start_time": "2020-10-31T21:36:54.718543Z"
    }
   },
   "outputs": [],
   "source": [
    "ids = np.random.randint(low=0, high=index2word.size, size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:13.293348Z",
     "start_time": "2020-10-31T21:36:54.725096Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_reduced = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:13.302692Z",
     "start_time": "2020-10-31T21:37:13.296054Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tsne_embeddings(embeddings, annotations):\n",
    "\n",
    "    trace = plotly.graph_objs.Scattergl(\n",
    "        x = embeddings[:, 0],\n",
    "        y = embeddings[:, 1],\n",
    "        name = 'Embedding',\n",
    "        mode = 'markers',\n",
    "\n",
    "        marker = dict(\n",
    "            colorscale='Viridis',\n",
    "            size = 6,\n",
    "            line = dict(width = 0.5),\n",
    "            opacity=0.75\n",
    "        ),\n",
    "        text=annotations\n",
    "    )\n",
    "\n",
    "    layout = dict(\n",
    "        title = \"Word2Vec 2D TSNE Embeddings\",\n",
    "        yaxis = dict(zeroline = False),\n",
    "        xaxis = dict(zeroline = False),\n",
    "        hovermode = 'closest',\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "\n",
    "    return plotly.graph_objs.Figure(data=[trace], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:14.534175Z",
     "start_time": "2020-10-31T21:37:13.305654Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tsne_embeddings(embeddings_reduced, index2word[ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:34:05.038696Z",
     "start_time": "2020-10-31T13:34:05.025442Z"
    }
   },
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "будем строить эмбеддинг предложения простым средним векторов слов. какие еще возможны варианты?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:14.538417Z",
     "start_time": "2020-10-31T21:37:14.535645Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_text(text, word2index, word_embeddings):\n",
    "    return np.array([\n",
    "        word_embeddings[word2index[word]] for word in text \n",
    "        if word in word2index and word not in stopword_set\n",
    "    ]).mean(0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:14.552005Z",
     "start_time": "2020-10-31T21:37:14.541266Z"
    }
   },
   "outputs": [],
   "source": [
    "word2index = {word: i for i, word in enumerate(index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:16.681582Z",
     "start_time": "2020-10-31T21:37:14.553722Z"
    }
   },
   "outputs": [],
   "source": [
    "talk2vec = np.concatenate([embed_text(doc, word2index, embeddings) for doc in cleared_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:16.688720Z",
     "start_time": "2020-10-31T21:37:16.683682Z"
    }
   },
   "outputs": [],
   "source": [
    "normed_talk2vec = normalize(talk2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:16.708370Z",
     "start_time": "2020-10-31T21:37:16.691154Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = normed_talk2vec @ normed_talk2vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:20.103621Z",
     "start_time": "2020-10-31T21:37:16.710171Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.96\n",
    "\n",
    "for i in range(similarities.shape[0]):\n",
    "    for j in range(similarities.shape[1]):\n",
    "        if j > i:\n",
    "            if similarities[i, j] > threshold:\n",
    "                print(f\"({i}, {j}) {similarities[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:20.112896Z",
     "start_time": "2020-10-31T21:37:20.106096Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((78, 116))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:20.121334Z",
     "start_time": "2020-10-31T21:37:20.116584Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((276, 1713))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:34.762075Z",
     "start_time": "2020-10-31T21:37:20.125996Z"
    }
   },
   "outputs": [],
   "source": [
    "talk2vec_reduced = TSNE(n_components=2, random_state=SEED).fit_transform(talk2vec)\n",
    "\n",
    "plot_tsne_embeddings(talk2vec_reduced, df.talk.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:36.994816Z",
     "start_time": "2020-10-31T21:37:34.764050Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(cleared_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:37.002050Z",
     "start_time": "2020-10-31T21:37:36.997290Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:37.022700Z",
     "start_time": "2020-10-31T21:37:37.004922Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = dictionary[0] # так надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:38.295168Z",
     "start_time": "2020-10-31T21:37:37.027482Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in cleared_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:37:38.322971Z",
     "start_time": "2020-10-31T21:37:38.297118Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T14:40:52.249531Z",
     "start_time": "2020-11-01T14:38:52.333512Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    chunksize=200,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=20,\n",
    "    passes=20,\n",
    "    random_state=SEED,\n",
    "    per_word_topics=True,\n",
    "    eval_every=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T21:39:51.028708Z",
     "start_time": "2020-10-31T21:39:50.998422Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица тем:  $$\\Theta \\left( T \\times \\vert V \\vert \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:05:01.375799Z",
     "start_time": "2020-11-01T09:05:01.299681Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = lda_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:14:17.535063Z",
     "start_time": "2020-11-01T09:14:17.528382Z"
    }
   },
   "outputs": [],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица документов: $$\\Phi \\left(\\vert D \\vert \\times  T \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T10:56:37.304413Z",
     "start_time": "2020-11-01T10:56:37.300383Z"
    }
   },
   "outputs": [],
   "source": [
    "document_topics = lda_model.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T10:56:40.848662Z",
     "start_time": "2020-11-01T10:56:37.962902Z"
    }
   },
   "outputs": [],
   "source": [
    "phi = gensim.matutils.corpus2dense(document_topics, num_terms=num_topics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T10:56:41.358985Z",
     "start_time": "2020-11-01T10:56:41.354883Z"
    }
   },
   "outputs": [],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск похожих документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:34:06.791326Z",
     "start_time": "2020-11-01T09:34:06.766766Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(phi, phi)\n",
    "\n",
    "similarities.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:33:47.304694Z",
     "start_time": "2020-11-01T09:33:47.180703Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.text.apply(lambda x: 'машинное обучение' in x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:34:16.372514Z",
     "start_time": "2020-11-01T09:34:16.368939Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_id = 540\n",
    "top_k = 20\n",
    "\n",
    "sorted_doc_ids = similarities[doc_id].argsort()[::-1][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T09:34:17.121566Z",
     "start_time": "2020-11-01T09:34:17.103105Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "pd.DataFrame({'neighbor': df.talk.values[sorted_doc_ids], 'similarity': similarities[doc_id, sorted_doc_ids]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое описание документов (суммаризация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T14:35:29.251019Z",
     "start_time": "2020-11-01T14:35:29.187220Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize(document, model, top_k):\n",
    "    tokenized_sentences = []\n",
    "    sentences = []\n",
    "    for sentence in razdel.sentenize(document):\n",
    "        sentences.append(sentence.text)\n",
    "        lemmatized_tokens = [lemmatize(token) for token in re.findall(TOKEN_PATTERN, sentence.text.lower())]\n",
    "        tokenized_sentences.append(\n",
    "            [token for token in lemmatized_tokens if token not in stopword_set]\n",
    "        )\n",
    "    \n",
    "    tokenized_document = [token for sentence in tokenized_sentences for token in sentence]\n",
    "    bow = dictionary.doc2bow(tokenized_document)\n",
    "    document_topics = [model.get_document_topics(bow)]\n",
    "    document_vector = gensim.matutils.corpus2dense(document_topics, num_terms=model.num_topics).reshape(1, -1)\n",
    "    \n",
    "    sentence_bows = [dictionary.doc2bow(sentence) for sentence in tokenized_sentences]\n",
    "    sentence_topics = model.get_document_topics(sentence_bows)\n",
    "    sentence_vectors = gensim.matutils.corpus2dense(sentence_topics, num_terms=model.num_topics).T\n",
    "    \n",
    "    relevance = cosine_similarity(document_vector, sentence_vectors)[0]\n",
    "    most_relevant_sentences = relevance.argsort()[::-1][:top_k]\n",
    "    \n",
    "    return \" \".join([sentence for i, sentence in enumerate(sentences) if i in most_relevant_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T14:37:17.582729Z",
     "start_time": "2020-11-01T14:37:17.260951Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize(df.text.values[540], lda_model, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T11:12:40.114615Z",
     "start_time": "2020-11-01T11:12:40.109651Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text.values[540]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация тематической модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
